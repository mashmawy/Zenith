# Modern LLM Training Framework

An Experimental, scalable framework for training large language models from scratch. Includes state-of-the-art architectural improvements and supports training on everything from a single CPU to multi-node GPU clusters.

## ðŸŒŸ Features

### Architecture
- âœ… **RoPE (Rotary Position Embeddings)** - Better position encoding
- âœ… **SwiGLU Activation** - Improved over ReLU/GELU  
- âœ… **RMSNorm** - More efficient than LayerNorm
- âœ… **Grouped Query Attention (GQA)** - Reduces memory footprint
- âœ… **Flash Attention** - Memory-efficient attention (when available)
- âœ… **Pre-normalization** - Improved training stability

### Training Capabilities
- âœ… Single CPU/GPU training
- âœ… Multi-GPU training (DDP)
- âœ… Multi-node distributed training
- âœ… Mixed precision training (FP16/BF16)
- âœ… Gradient accumulation
- âœ… Automatic checkpointing 
- âœ… PyTorch 2.0 compile support

### Data Processing
- âœ… Multi-format support (.txt, .md, .json)
- âœ… Multiple tokenizer types (BPE, WordPiece, TikToken)
- âœ… Automatic train/validation split
- âœ… Overlapping chunks for context preservation
- âœ… Multiprocessing for fast preparation

## ðŸ“¦ Installation

### Prerequisites
- Python 3.8+
- CUDA Toolkit 11.8+ (for GPU training)
- 8GB+ RAM (16GB+ recommended)

### Setup

```bash
# Clone or download the framework files
mkdir llm_training && cd llm_training

# Install dependencies
pip install -r requirements.txt

# For GPU support (if needed)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

# Verify installation
python utils.py setup
```

## ðŸš€ Quick Start

### 1. Prepare Your Data

```bash
# Put your text files in a folder
mkdir raw_text_data
# Add your .txt, .md, or .json files

# Process the data
python data_prep_tool.py \
    --input_dir ./raw_text_data \
    --output_dir ./processed_data \
    --vocab_size 32000 \
    --max_length 2048
```

### 2. Train Your Model

**Tiny model (testing):**
```bash
python train.py \
    --data_dir ./processed_data \
    --hidden_size 256 \
    --num_layers 6 \
    --batch_size 8 \
    --max_steps 10000
```

**Small model (single GPU):**
```bash
python train.py \
    --data_dir ./processed_data \
    --hidden_size 768 \
    --num_layers 12 \
    --batch_size 32 \
    --max_steps 100000 \
    --mixed_precision
```

**Multi-GPU training:**
```bash
torchrun --nproc_per_node=4 train.py \
    --data_dir ./processed_data \
    --hidden_size 1024 \
    --num_layers 16 \
    --batch_size 128 \
    --micro_batch_size 32 \
    --mixed_precision
```

### 3. Monitor Training

```bash
# Watch training progress
python utils.py monitor --checkpoint_dir ./checkpoints
 
```

## ðŸ“š Documentation

### File Structure

```
llm_training/
â”œâ”€â”€ data_prep_tool.py      # Data preprocessing tool
â”œâ”€â”€ model.py            # Model architecture
â”œâ”€â”€ train.py            # Training framework
â”œâ”€â”€ utils.py            # Utility scripts
â”œâ”€â”€ requirements.txt    # Dependencies
â”œâ”€â”€ README.md           # This file
â”‚
â”œâ”€â”€ raw_text_data/      # Your source text files
â”œâ”€â”€ processed_data/     # Tokenized training data
â”‚   â”œâ”€â”€ tokenizer.json
â”‚   â”œâ”€â”€ dataset/
â”‚   â””â”€â”€ metadata.json
â””â”€â”€ checkpoints/        # Model checkpoints
    â”œâ”€â”€ checkpoint_step_1000.pt
    â”œâ”€â”€ checkpoint_step_2000.pt
    â””â”€â”€ ...
```

### Model Configurations

| Size | Hidden | Layers | Heads | Params | GPU Memory | Training Time* |
|------|--------|--------|-------|--------|------------|----------------|
| Tiny | 256 | 6 | 8 | 20M | 2GB | 2 days |
| Small | 768 | 12 | 12 | 125M | 8GB | 5 days |
| Medium | 1024 | 24 | 16 | 350M | 16GB | 2 weeks |
| Large | 1536 | 24 | 16 | 800M | 24GB | 1 month |
| XL | 2048 | 32 | 32 | 1.5B | 40GB | 2 months |

*On single RTX 3090 with mixed precision

### Command Line Arguments

#### data_prep_tool.py

```bash
--input_dir          # Directory with text files (required)
--output_dir         # Output directory (required)
--tokenizer_type     # bpe, wordpiece, or tiktoken (default: bpe)
--vocab_size         # Vocabulary size (default: 32000)
--max_length         # Max sequence length (default: 2048)
--stride             # Overlap between chunks (default: 1024)
--num_workers        # CPU workers for processing (default: 4)
```

#### train.py

**Model Architecture:**
```bash
--vocab_size         # Vocabulary size (default: 32000)
--hidden_size        # Hidden dimension (default: 768)
--num_layers         # Number of layers (default: 12)
--num_heads          # Attention heads (default: 12)
--num_kv_heads       # KV heads for GQA (default: same as num_heads)
```

**Training:**
```bash
--data_dir           # Processed data directory (required)
--save_dir           # Checkpoint save directory (default: ./checkpoints)
--batch_size         # Total batch size (default: 32)
--micro_batch_size   # Batch size per step (for gradient accumulation)
--max_steps          # Training steps (default: 100000)
--learning_rate      # Learning rate (default: 3e-4)
--warmup_steps       # Warmup steps (default: 2000)
--grad_clip          # Gradient clipping (default: 1.0)
```

**System:**
```bash
--mixed_precision    # Enable FP16 training
--compile_model      # Use torch.compile (PyTorch 2.0+)
--checkpoint         # Resume from checkpoint
```

**Logging:**
```bash 
--log_interval       # Steps between logs (default: 100)
--eval_interval      # Steps between validation (default: 500)
--