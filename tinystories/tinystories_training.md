# Training on TinyStories Dataset
## Complete Guide with Ready-to-Run Scripts

---

## Dataset Overview

**TinyStories** is a synthetic dataset of simple stories designed for training small language models.

### Dataset Statistics

```
Name: roneneldan/TinyStories
Size: ~2.1 million short stories
Total tokens: ~500 million
Language: English (simple vocabulary)
Source: GPT-3.5 and GPT-4 generated
Purpose: Train small models that can generate coherent stories

Splits:
- train: 2,119,719 stories
- validation: 21,990 stories

Story characteristics:
- Simple vocabulary (words 3-4 year olds understand)
- Clear narrative structure
- 200-300 words per story
- Topics: animals, family, friends, adventures
```

### Why TinyStories is Great for Training

‚úÖ **Perfect for learning**: Simple, coherent text
‚úÖ **Good size**: Large enough to train real models
‚úÖ **Fast training**: Simpler than complex datasets
‚úÖ **Quality content**: Human-verified generation
‚úÖ **Immediate results**: Models learn to write stories quickly

---

## Installation & Setup

### Install Dependencies

```bash
# Core dependencies
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
pip install transformers>=4.36.0
pip install datasets>=2.14.0
pip install tokenizers>=0.13.0
pip install accelerate>=0.24.0
pip install wandb
pip install tqdm
```

### Verify Installation

```bash
python -c "
from datasets import load_dataset
print('‚úÖ datasets installed')
import torch
print(f'‚úÖ PyTorch {torch.__version__}')
print(f'‚úÖ CUDA available: {torch.cuda.is_available()}')
"
```

---

## Script 1: Prepare TinyStories Data

```python
"""
prepare_tinystories.py
Download and prepare TinyStories dataset for training
"""

from datasets import load_dataset
from tokenizers import Tokenizer, models, trainers, pre_tokenizers, processors
from tqdm import tqdm
import os
import json


def download_tinystories():
    """
    Download TinyStories dataset from Hugging Face
    """
    print("Downloading TinyStories dataset...")
    print("This may take a few minutes on first run...")
    
    # Load dataset
    dataset = load_dataset("roneneldan/TinyStories")
    
    print(f"\n‚úÖ Dataset downloaded!")
    print(f"Train examples: {len(dataset['train']):,}")
    print(f"Validation examples: {len(dataset['validation']):,}")
    
    # Show sample
    print("\nüìñ Sample story:")
    print("="*60)
    print(dataset['train'][0]['text'][:500] + "...")
    print("="*60)
    
    return dataset


def train_tokenizer_on_tinystories(dataset, vocab_size=8000, save_path="./tinystories_tokenizer"):
    """
    Train a BPE tokenizer specifically on TinyStories
    
    Uses smaller vocab since stories use simple language
    """
    print(f"\nTraining tokenizer with vocab_size={vocab_size}...")
    
    # Create BPE tokenizer
    tokenizer = Tokenizer(models.BPE(unk_token="<unk>"))
    
    # Configure
    tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False)
    tokenizer.post_processor = processors.ByteLevel(trim_offsets=False)
    
    # Trainer
    trainer = trainers.BpeTrainer(
        vocab_size=vocab_size,
        min_frequency=2,
        special_tokens=["<pad>", "<unk>", "<s>", "</s>"],
        show_progress=True,
    )
    
    # Text iterator
    def batch_iterator(batch_size=1000):
        for i in range(0, len(dataset['train']), batch_size):
            batch = dataset['train'][i:i+batch_size]
            yield batch['text']
    
    # Train
    tokenizer.train_from_iterator(batch_iterator(), trainer=trainer)
    
    # Save
    os.makedirs(save_path, exist_ok=True)
    tokenizer.save(f"{save_path}/tokenizer.json")
    
    print(f"‚úÖ Tokenizer saved to {save_path}")
    print(f"Vocabulary size: {tokenizer.get_vocab_size():,}")
    
    return tokenizer


def tokenize_dataset(dataset, tokenizer, max_length=512, save_path="./tinystories_processed"):
    """
    Tokenize all stories and prepare for training
    
    max_length=512 is good for TinyStories (stories are ~200-300 words)
    """
    print(f"\nTokenizing dataset (max_length={max_length})...")
    
    def tokenize_function(examples):
        # Tokenize
        tokens = tokenizer.encode_batch(examples['text'])
        
        # Convert to IDs and create chunks
        all_input_ids = []
        all_attention_masks = []
        
        for token in tokens:
            ids = token.ids
            
            # Truncate or pad to max_length
            if len(ids) > max_length:
                ids = ids[:max_length]
            
            # Create attention mask
            attention_mask = [1] * len(ids)
            
            # Pad if needed
            if len(ids) < max_length:
                padding_length = max_length - len(ids)
                ids = ids + [0] * padding_length
                attention_mask = attention_mask + [0] * padding_length
            
            all_input_ids.append(ids)
            all_attention_masks.append(attention_mask)
        
        return {
            'input_ids': all_input_ids,
            'attention_mask': all_attention_masks
        }
    
    # Tokenize
    tokenized_dataset = dataset.map(
        tokenize_function,
        batched=True,
        batch_size=1000,
        remove_columns=['text'],
        desc="Tokenizing stories"
    )
    
    # Save
    os.makedirs(save_path, exist_ok=True)
    tokenized_dataset.save_to_disk(save_path)
    
    # Save metadata
    metadata = {
        'dataset': 'roneneldan/TinyStories',
        'vocab_size': tokenizer.get_vocab_size(),
        'max_length': max_length,
        'train_examples': len(tokenized_dataset['train']),
        'val_examples': len(tokenized_dataset['validation']),
    }
    
    with open(f"{save_path}/metadata.json", 'w') as f:
        json.dump(metadata, f, indent=2)
    
    print(f"‚úÖ Tokenized dataset saved to {save_path}")
    print(f"Train examples: {len(tokenized_dataset['train']):,}")
    print(f"Validation examples: {len(tokenized_dataset['validation']):,}")
    
    return tokenized_dataset


def main():
    """
    Complete pipeline: Download ‚Üí Train Tokenizer ‚Üí Tokenize ‚Üí Save
    """
    print("="*60)
    print("TinyStories Data Preparation Pipeline")
    print("="*60)
    
    # Step 1: Download dataset
    dataset = download_tinystories()
    
    # Step 2: Train tokenizer
    # Using smaller vocab_size=8000 since TinyStories has simple vocabulary
    tokenizer = train_tokenizer_on_tinystories(
        dataset, 
        vocab_size=8000,
        save_path="./tinystories_tokenizer"
    )
    
    # Step 3: Tokenize and save
    # Using max_length=512 since stories are short
    tokenized_dataset = tokenize_dataset(
        dataset,
        tokenizer,
        max_length=512,
        save_path="./tinystories_processed"
    )
    
    print("\n" + "="*60)
    print("‚úÖ Data preparation complete!")
    print("="*60)
    print("\nNext steps:")
    print("1. Run: python train_tinystories.py")
    print("2. Monitor training with WandB")
    print("3. Generate stories with trained model")


if __name__ == "__main__":
    main()
```

---

## Script 2: Train on TinyStories

```python
"""
train_tinystories.py
Train a small language model on TinyStories dataset
"""

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from datasets import load_from_disk
from tokenizers import Tokenizer
import os
import json
from tqdm import tqdm
import wandb

# Import your model
from model import LLMModel, ModelConfig


class TinyStoriesTrainer:
    """
    Trainer specifically configured for TinyStories
    """
    
    def __init__(
        self,
        data_dir="./tinystories_processed",
        tokenizer_dir="./tinystories_tokenizer",
        output_dir="./tinystories_model",
        
        # Model config
        hidden_size=384,      # Smaller model for TinyStories
        num_layers=6,         # Fewer layers
        num_heads=6,
        
        # Training config
        batch_size=64,        # Can use larger batch with smaller model
        learning_rate=3e-4,
        max_steps=50000,      # ~50k steps is enough
        warmup_steps=1000,
        
        # System
        device="cuda" if torch.cuda.is_available() else "cpu",
        mixed_precision=True,
        
        # Logging
        use_wandb=True,
        log_interval=100,
        eval_interval=1000,
        save_interval=100,  # Save every 100 steps!
        keep_last_n_checkpoints=10,  # Keep only last 10 checkpoints
    ):
        self.data_dir = data_dir
        self.output_dir = output_dir
        self.mixed_precision = mixed_precision
        self.log_interval = log_interval
        self.eval_interval = eval_interval
        self.save_interval = save_interval
        self.keep_last_n_checkpoints = keep_last_n_checkpoints
        
        # Force GPU if available
        if torch.cuda.is_available():
            self.device = torch.device("cuda")
            print(f"‚úÖ Using GPU: {torch.cuda.get_device_name(0)}")
            print(f"   GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")
        else:
            self.device = torch.device("cpu")
            print("‚ö†Ô∏è  WARNING: No GPU detected! Training will be VERY slow on CPU.")
            response = input("Continue on CPU? (yes/no): ")
            if response.lower() != 'yes':
                exit(1)
        
        # Load metadata
        with open(f"{data_dir}/metadata.json", 'r') as f:
            metadata = json.load(f)
        
        # Model config
        self.model_config = ModelConfig(
            vocab_size=metadata['vocab_size'],
            hidden_size=hidden_size,
            num_layers=num_layers,
            num_heads=num_heads,
            max_position_embeddings=metadata['max_length'],
        )
        
        # Training config
        self.batch_size = batch_size
        self.learning_rate = learning_rate
        self.max_steps = max_steps
        self.warmup_steps = warmup_steps
        
        # Initialize
        self.step = 0
        self.epoch = 0
        self.best_val_loss = float('inf')
        
        self.setup_data()
        self.setup_model()
        self.setup_optimizer()
        
        if mixed_precision and self.device.type == "cuda":
            self.scaler = torch.cuda.amp.GradScaler()
        else:
            self.scaler = None
        
        # WandB
        self.use_wandb = use_wandb
        if use_wandb:
            wandb.init(
                project="tinystories-training",
                config={
                    "hidden_size": hidden_size,
                    "num_layers": num_layers,
                    "batch_size": batch_size,
                    "learning_rate": learning_rate,
                    "max_steps": max_steps,
                },
                resume="allow"  # Allow resuming WandB runs
            )
        
        print("\n" + "="*60)
        print("TinyStories Training Configuration")
        print("="*60)
        print(f"Device: {device}")
        print(f"Model parameters: {self.count_parameters():,}")
        print(f"Training examples: {len(self.train_dataset):,}")
        print(f"Validation examples: {len(self.val_dataset):,}")
        print(f"Batch size: {batch_size}")
        print(f"Max steps: {max_steps}")
        print(f"Mixed precision: {mixed_precision}")
        print("="*60 + "\n")
    
    def setup_data(self):
        """Load tokenized dataset"""
        print("Loading tokenized dataset...")
        dataset = load_from_disk(self.data_dir)
        
        self.train_dataset = dataset['train']
        self.val_dataset = dataset['validation']
        
        self.train_loader = DataLoader(
            self.train_dataset,
            batch_size=self.batch_size,
            shuffle=True,
            num_workers=4,
            pin_memory=True
        )
        
        self.val_loader = DataLoader(
            self.val_dataset,
            batch_size=self.batch_size,
            shuffle=False,
            num_workers=4,
            pin_memory=True
        )
    
    def setup_model(self):
        """Initialize model"""
        print("Initializing model...")
        self.model = LLMModel(self.model_config)
        self.model = self.model.to(self.device)
        self.model.train()
    
    def setup_optimizer(self):
        """Setup AdamW optimizer"""
        # Separate parameters
        decay_params = []
        nodecay_params = []
        
        for name, param in self.model.named_parameters():
            if param.requires_grad:
                if 'bias' in name or 'norm' in name:
                    nodecay_params.append(param)
                else:
                    decay_params.append(param)
        
        self.optimizer = torch.optim.AdamW([
            {'params': decay_params, 'weight_decay': 0.1},
            {'params': nodecay_params, 'weight_decay': 0.0}
        ], lr=self.learning_rate, betas=(0.9, 0.95))
    
    def count_parameters(self):
        """Count trainable parameters"""
        return sum(p.numel() for p in self.model.parameters() if p.requires_grad)
    
    def get_lr(self, step):
        """Cosine learning rate schedule with warmup"""
        if step < self.warmup_steps:
            return self.learning_rate * step / self.warmup_steps
        
        if step > self.max_steps:
            return self.learning_rate * 0.1
        
        progress = (step - self.warmup_steps) / (self.max_steps - self.warmup_steps)
        return self.learning_rate * 0.1 + 0.9 * self.learning_rate * (1 + math.cos(math.pi * progress)) / 2
    
    def train_step(self, batch):
        """Single training step"""
        input_ids = batch['input_ids'].to(self.device)
        attention_mask = batch['attention_mask'].to(self.device)
        
        # Forward pass
        if self.scaler is not None:
            with torch.cuda.amp.autocast():
                logits, loss = self.model(input_ids, attention_mask, labels=input_ids)
        else:
            logits, loss = self.model(input_ids, attention_mask, labels=input_ids)
        
        # Backward pass
        if self.scaler is not None:
            self.scaler.scale(loss).backward()
            self.scaler.unscale_(self.optimizer)
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
            self.scaler.step(self.optimizer)
            self.scaler.update()
        else:
            loss.backward()
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
            self.optimizer.step()
        
        self.optimizer.zero_grad(set_to_none=True)
        
        return loss.item()
    
    @torch.no_grad()
    def evaluate(self):
        """Evaluate on validation set"""
        self.model.eval()
        total_loss = 0
        num_batches = 0
        
        for batch in self.val_loader:
            if num_batches >= 50:  # Limit eval batches
                break
            
            input_ids = batch['input_ids'].to(self.device)
            attention_mask = batch['attention_mask'].to(self.device)
            
            if self.scaler is not None:
                with torch.cuda.amp.autocast():
                    logits, loss = self.model(input_ids, attention_mask, labels=input_ids)
            else:
                logits, loss = self.model(input_ids, attention_mask, labels=input_ids)
            
            total_loss += loss.item()
            num_batches += 1
        
        self.model.train()
        return total_loss / num_batches
    
    def save_checkpoint(self):
        """Save model checkpoint with automatic cleanup of old checkpoints"""
        os.makedirs(self.output_dir, exist_ok=True)
        
        checkpoint = {
            'step': self.step,
            'epoch': self.epoch,
            'best_val_loss': self.best_val_loss,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'model_config': {
                'vocab_size': self.model_config.vocab_size,
                'hidden_size': self.model_config.hidden_size,
                'num_layers': self.model_config.num_layers,
                'num_heads': self.model_config.num_heads,
                'max_position_embeddings': self.model_config.max_position_embeddings,
            }
        }
        
        if self.scaler is not None:
            checkpoint['scaler_state_dict'] = self.scaler.state_dict()
        
        # Save checkpoint
        checkpoint_path = f"{self.output_dir}/checkpoint_step_{self.step}.pt"
        torch.save(checkpoint, checkpoint_path)
        
        # Save as latest
        latest_path = f"{self.output_dir}/checkpoint_latest.pt"
        torch.save(checkpoint, latest_path)
        
        print(f"üíæ Checkpoint saved: step {self.step}")
        
        # Cleanup old checkpoints (keep last N)
        self.cleanup_old_checkpoints()
    
    def cleanup_old_checkpoints(self):
        """Remove old checkpoints, keeping only the last N"""
        import glob
        
        # Get all checkpoint files (except latest and best)
        checkpoints = glob.glob(f"{self.output_dir}/checkpoint_step_*.pt")
        
        if len(checkpoints) <= self.keep_last_n_checkpoints:
            return
        
        # Sort by step number
        checkpoints.sort(key=lambda x: int(x.split('_step_')[1].split('.pt')[0]))
        
        # Remove oldest checkpoints
        to_remove = checkpoints[:-self.keep_last_n_checkpoints]
        for checkpoint in to_remove:
            os.remove(checkpoint)
            if self.step % 1000 == 0:  # Only print occasionally
                print(f"üóëÔ∏è  Removed old checkpoint: {os.path.basename(checkpoint)}")
    
    def load_checkpoint(self, checkpoint_path):
        """Load checkpoint and resume training"""
        print(f"\nüìÇ Loading checkpoint from {checkpoint_path}...")
        
        checkpoint = torch.load(checkpoint_path, map_location=self.device)
        
        # Restore model
        self.model.load_state_dict(checkpoint['model_state_dict'])
        
        # Restore optimizer
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        
        # Restore training state
        self.step = checkpoint['step']
        self.epoch = checkpoint.get('epoch', 0)
        self.best_val_loss = checkpoint.get('best_val_loss', float('inf'))
        
        # Restore scaler if using mixed precision
        if self.scaler is not None and 'scaler_state_dict' in checkpoint:
            self.scaler.load_state_dict(checkpoint['scaler_state_dict'])
        
        print(f"‚úÖ Resumed from step {self.step}")
        print(f"   Best validation loss: {self.best_val_loss:.4f}")
        print()
    
    def find_latest_checkpoint(self):
        """Find the latest checkpoint in output directory"""
        latest_path = f"{self.output_dir}/checkpoint_latest.pt"
        
        if os.path.exists(latest_path):
            return latest_path
        
        # Fallback: find highest step number
        import glob
        checkpoints = glob.glob(f"{self.output_dir}/checkpoint_step_*.pt")
        
        if not checkpoints:
            return None
        
        # Sort by step number and return latest
        checkpoints.sort(key=lambda x: int(x.split('_step_')[1].split('.pt')[0]))
        return checkpoints[-1]
    
    def train(self):
        """Main training loop"""
        print("Starting training...\n")
        
        import math
        import time
        
        losses = []
        start_time = time.time()
        
        while self.step < self.max_steps:
            for batch in self.train_loader:
                # Update learning rate
                lr = self.get_lr(self.step)
                for param_group in self.optimizer.param_groups:
                    param_group['lr'] = lr
                
                # Training step
                loss = self.train_step(batch)
                losses.append(loss)
                self.step += 1
                
                # Logging
                if self.step % self.log_interval == 0:
                    avg_loss = sum(losses) / len(losses)
                    elapsed = time.time() - start_time
                    tokens_per_sec = (self.batch_size * 512 * self.log_interval) / elapsed
                    
                    print(f"Step {self.step:6d}/{self.max_steps} | "
                          f"Loss: {avg_loss:.4f} | "
                          f"LR: {lr:.2e} | "
                          f"Tokens/s: {tokens_per_sec:,.0f}")
                    
                    if wandb.run is not None:
                        wandb.log({
                            'train/loss': avg_loss,
                            'train/lr': lr,
                            'train/tokens_per_sec': tokens_per_sec,
                            'step': self.step
                        })
                    
                    losses = []
                    start_time = time.time()
                
                # Evaluation
                if self.step % self.eval_interval == 0:
                    val_loss = self.evaluate()
                    
                    # Save best model
                    if val_loss < self.best_val_loss:
                        self.best_val_loss = val_loss
                        best_path = f"{self.output_dir}/checkpoint_best.pt"
                        torch.save({
                            'step': self.step,
                            'epoch': self.epoch,
                            'best_val_loss': self.best_val_loss,
                            'model_state_dict': self.model.state_dict(),
                            'optimizer_state_dict': self.optimizer.state_dict(),
                            'model_config': {
                                'vocab_size': self.model_config.vocab_size,
                                'hidden_size': self.model_config.hidden_size,
                                'num_layers': self.model_config.num_layers,
                                'num_heads': self.model_config.num_heads,
                                'max_position_embeddings': self.model_config.max_position_embeddings,
                            }
                        }, best_path)
                        print(f"‚≠ê New best model! Validation loss: {val_loss:.4f}")
                    else:
                        print(f"üìä Validation loss: {val_loss:.4f} (best: {self.best_val_loss:.4f})")
                    
                    if wandb.run is not None:
                        wandb.log({'val/loss': val_loss, 'val/best_loss': self.best_val_loss, 'step': self.step})
                
                # Save checkpoint
                if self.step % self.save_interval == 0:
                    self.save_checkpoint()
                
                if self.step >= self.max_steps:
                    break
            
            self.epoch += 1
        
        # Save final model
        self.save_checkpoint()
        print("\n" + "="*60)
        print("‚úÖ Training complete!")
        print(f"   Total steps: {self.step}")
        print(f"   Best validation loss: {self.best_val_loss:.4f}")
        print(f"   Final model: {self.output_dir}/checkpoint_step_{self.step}.pt")
        print(f"   Best model: {self.output_dir}/checkpoint_best.pt")
        print("="*60)


def main():
    import argparse
    
    parser = argparse.ArgumentParser(description="Train on TinyStories dataset")
    
    # Paths
    parser.add_argument("--data_dir", type=str, default="./tinystories_processed", 
                        help="Path to processed dataset")
    parser.add_argument("--output_dir", type=str, default="./tinystories_model",
                        help="Directory to save checkpoints")
    parser.add_argument("--resume", action="store_true", 
                        help="Resume training from latest checkpoint")
    parser.add_argument("--checkpoint", type=str, default=None, 
                        help="Path to specific checkpoint to resume from")
    
    # Model config
    parser.add_argument("--hidden_size", type=int, default=384,
                        help="Hidden dimension size (default: 384)")
    parser.add_argument("--num_layers", type=int, default=6,
                        help="Number of transformer layers (default: 6)")
    
    # Training config
    parser.add_argument("--batch_size", type=int, default=64,
                        help="Batch size (default: 64)")
    parser.add_argument("--max_steps", type=int, default=50000,
                        help="Maximum training steps (default: 50000)")
    parser.add_argument("--learning_rate", type=float, default=3e-4,
                        help="Learning rate (default: 3e-4)")
    
    # System
    parser.add_argument("--no_mixed_precision", action="store_true",
                        help="Disable mixed precision training")
    parser.add_argument("--no_wandb", action="store_true",
                        help="Disable Weights & Biases logging")
    
    # Checkpointing
    parser.add_argument("--save_interval", type=int, default=100, 
                        help="Save checkpoint every N steps (default: 100)")
    parser.add_argument("--keep_checkpoints", type=int, default=10, 
                        help="Keep last N checkpoints (default: 10)")
    
    args = parser.parse_args()
    
    # Verify GPU
    if not torch.cuda.is_available():
        print("\n" + "="*60)
        print("‚ö†Ô∏è  WARNING: CUDA not available!")
        print("="*60)
        print("GPU training is STRONGLY recommended for this task.")
        print("CPU training will be 50-100x slower.")
        print("\nTo enable GPU:")
        print("1. Install CUDA toolkit")
        print("2. Reinstall PyTorch with CUDA support:")
        print("   pip install torch --index-url https://download.pytorch.org/whl/cu118")
        print("="*60 + "\n")
    
    # Create trainer
    trainer = TinyStoriesTrainer(
        data_dir=args.data_dir,
        output_dir=args.output_dir,
        hidden_size=args.hidden_size,
        num_layers=args.num_layers,
        batch_size=args.batch_size,
        max_steps=args.max_steps,
        learning_rate=args.learning_rate,
        mixed_precision=not args.no_mixed_precision,
        use_wandb=not args.no_wandb,
        save_interval=args.save_interval,
        keep_last_n_checkpoints=args.keep_checkpoints,
    )
    
    # Resume if requested
    if args.resume or args.checkpoint:
        if args.checkpoint:
            # Resume from specific checkpoint
            trainer.load_checkpoint(args.checkpoint)
        else:
            # Resume from latest checkpoint
            latest_checkpoint = trainer.find_latest_checkpoint()
            if latest_checkpoint:
                trainer.load_checkpoint(latest_checkpoint)
            else:
                print("‚ö†Ô∏è  No checkpoint found to resume from. Starting fresh...")
    
    # Train
    try:
        trainer.train()
    except KeyboardInterrupt:
        print("\n\n‚ö†Ô∏è  Training interrupted by user!")
        print("Saving checkpoint before exit...")
        trainer.save_checkpoint()
        print("‚úÖ Checkpoint saved. You can resume with --resume flag")
        if wandb.run is not None:
            wandb.finish()
    except Exception as e:
        print(f"\n\n‚ùå Error during training: {e}")
        print("Saving checkpoint before exit...")
        trainer.save_checkpoint()
        print("‚úÖ Checkpoint saved")
        if wandb.run is not None:
            wandb.finish()
        raise


if __name__ == "__main__":
    main()
```

---

## Script 3: Generate Stories

```python
"""
generate_stories.py
Generate new stories with trained model
"""

import torch
from tokenizers import Tokenizer
import json

from model import LLMModel, ModelConfig


def load_trained_model(checkpoint_path, tokenizer_path):
    """Load trained model and tokenizer"""
    # Load checkpoint
    checkpoint = torch.load(checkpoint_path, map_location='cpu')
    
    # Create model
    config = ModelConfig(**checkpoint['model_config'])
    model = LLMModel(config)
    model.load_state_dict(checkpoint['model_state_dict'])
    model.eval()
    
    # Load tokenizer
    tokenizer = Tokenizer.from_file(f"{tokenizer_path}/tokenizer.json")
    
    return model, tokenizer


def generate_story(
    model,
    tokenizer,
    prompt="Once upon a time",
    max_length=300,
    temperature=0.8,
    top_p=0.95,
    device="cuda" if torch.cuda.is_available() else "cpu"
):
    """
    Generate a story from a prompt
    """
    model = model.to(device)
    
    # Tokenize prompt
    encoded = tokenizer.encode(prompt)
    input_ids = torch.tensor([encoded.ids]).to(device)
    
    # Generate
    generated_ids = input_ids[0].tolist()
    
    with torch.no_grad():
        for _ in range(max_length):
            # Forward pass
            logits, _ = model(input_ids)
            
            # Get next token logits
            next_token_logits = logits[0, -1, :] / temperature
            
            # Top-p sampling
            sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)
            cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)
            
            # Remove tokens with cumulative probability above threshold
            sorted_indices_to_remove = cumulative_probs > top_p
            sorted_indices_to_remove[1:] = sorted_indices_to_remove[:-1].clone()
            sorted_indices_to_remove[0] = 0
            
            indices_to_remove = sorted_indices[sorted_indices_to_remove]
            next_token_logits[indices_to_remove] = float('-inf')
            
            # Sample
            probs = torch.softmax(next_token_logits, dim=-1)
            next_token = torch.multinomial(probs, num_samples=1)
            
            # Add to generated
            generated_ids.append(next_token.item())
            
            # Update input_ids
            input_ids = torch.tensor([generated_ids]).to(device)
            
            # Stop at end token (if you have one)
            if next_token.item() == 3:  # </s> token
                break
    
    # Decode
    story = tokenizer.decode(generated_ids)
    return story


def main():
    import argparse
    
    parser = argparse.ArgumentParser()
    parser.add_argument("--checkpoint", required=True, help="Path to checkpoint")
    parser.add_argument("--tokenizer", default="./tinystories_tokenizer")
    parser.add_argument("--prompt", default="Once upon a time")
    parser.add_argument("--num_stories", type=int, default=5)
    parser.add_argument("--temperature", type=float, default=0.8)
    
    args = parser.parse_args()
    
    # Load model
    print("Loading model...")
    model, tokenizer = load_trained_model(args.checkpoint, args.tokenizer)
    print("‚úÖ Model loaded\n")
    
    # Generate stories
    for i in range(args.num_stories):
        print(f"Story {i+1}:")
        print("="*60)
        story = generate_story(
            model,
            tokenizer,
            prompt=args.prompt,
            temperature=args.temperature
        )
        print(story)
        print("="*60 + "\n")


if __name__ == "__main__":
    main()
```

---

## Complete Training Pipeline

### Step 1: Prepare Data

```bash
python prepare_tinystories.py
```

**What happens:**
- Downloads TinyStories (2.1M stories)
- Trains BPE tokenizer (vocab_size=8000)
- Tokenizes all stories (max_length=512)
- Saves to `./tinystories_processed/`

**Time:** ~10-20 minutes (first run only)

### Step 2: Train Model

```bash
# Default (small model, fast training)
python train_tinystories.py

# Larger model (better quality)
python train_tinystories.py \
    --hidden_size 512 \
    --num_layers 8 \
    --batch_size 48 \
    --max_steps 100000

# Custom configuration
python train_tinystories.py \
    --data_dir ./tinystories_processed \
    --output_dir ./my_tinystories_model \
    --hidden_size 384 \
    --num_layers 6 \
    --batch_size 64 \
    --max_steps 50000
```

**Training time:**
- Small model (384, 6 layers): ~3-4 hours on RTX 3080
- Medium model (512, 8 layers): ~6-8 hours
- Large model (768, 12 layers): ~12-16 hours

### Step 3: Generate Stories

```bash
python generate_stories.py \
    --checkpoint ./tinystories_model/checkpoint_step_50000.pt \
    --prompt "Once upon a time, there was a brave little mouse" \
    --num_stories 5
```

---

## Expected Results

### After 10,000 steps:
```
Story: Once upon a time there was a little girl. She liked to play. One day she saw a big dog...
```
‚úì Basic grammar
‚úì Simple sentences
‚ö†Ô∏è Limited coherence

### After 30,000 steps:
```
Story: Once upon a time, there was a little girl named Lucy. She loved to play in the park with her friends. One sunny day, Lucy found a shiny red ball...
```
‚úì Better coherence
‚úì Named characters
‚úì Simple narratives

### After 50,000+ steps:
```
Story: Once upon a time, in a cozy little house, there lived a brave mouse named Max. Max loved adventures and exploring new places. One day, Max decided to visit the big forest near his home. 

As Max walked through the forest, he met a friendly bird named Bella. "Hello!" chirped Bella. "Where are you going?"

"I'm looking for the magical tree," said Max. "My grandma told me stories about it."

Bella smiled. "I know where it is! Follow me!" Together, they walked deeper into the forest...
```
‚úì Excellent coherence
‚úì Dialogue
‚úì Story structure
‚úì Character development

---

## Model Size Recommendations

| Config | Parameters | GPU Memory | Training Time | Quality |
|--------|-----------|------------|---------------|---------|
| **Tiny** | hidden=256, layers=4 | ~18M | 2GB | 2 hours | Basic |
| **Small** | hidden=384, layers=6 | ~43M | 3GB | 4 hours | Good |
| **Medium** | hidden=512, layers=8 | ~76M | 5GB | 8 hours | Very Good |
| **Large** | hidden=768, layers=12 | ~162M | 8GB | 16 hours | Excellent |

---

## Tips for Best Results

1. **Start Small**: Train tiny model first to verify pipeline works
2. **Monitor Loss**: Should decrease to ~2.5-3.0
3. **Use WandB**: Track experiments and compare runs
4. **Checkpoint Often**: Save every 5,000 steps
5. **Generate During Training**: Test quality at different steps
6. **Temperature**: Lower (0.7) = more coherent, Higher (0.9) = more creative

---

## Troubleshooting

**Out of Memory:**
```bash
# Reduce batch size
python train_tinystories.py --batch_size 32

# Smaller model
python train_tinystories.py --hidden_size 256 --num_layers 4
```

**Slow Training:**
```bash
# Verify GPU is being used
python -c "import torch; print(torch.cuda.is_available())"

# Enable mixed precision (should be default)
# Check in trainer: mixed_precision=True
```

**Poor Quality Stories:**
- Train longer (50k+ steps)
- Use larger model
- Check validation loss (should be < 3.0)
- Adjust generation temperature

---

This is a complete, working pipeline specifically tuned for TinyStories! üéâ